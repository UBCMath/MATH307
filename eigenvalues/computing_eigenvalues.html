
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Computing Eigenvalues &#8212; Applied Linear Algebra</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e8f53015daec13862f6db5e763c41738.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/main.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/main.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Discrete Fourier Transform" href="../dft/dft.html" />
    <link rel="prev" title="Singular Value Decomposition" href="svd.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/nn.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Applied Linear Algebra</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  Linear Equations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../linear_equations/lu.html">
   LU Decomposition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../linear_equations/error_analysis.html">
   Error Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../linear_equations/interpolation.html">
   Interpolation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../linear_equations/differential_equations.html">
   Differential Equations
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Orthogonality
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../orthogonality/subspaces.html">
   Subspaces
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../orthogonality/projection.html">
   Orthogonal Projection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../orthogonality/qr.html">
   QR Decomposition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../orthogonality/least_squares.html">
   Least Squares Approximation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Eigenvalues
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="diagonalization.html">
   Diagonalization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="svd.html">
   Singular Value Decomposition
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Computing Eigenvalues
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Dicrete Fourier Transform
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../dft/dft.html">
   Discrete Fourier Transform
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dft/frequency.html">
   Frequency, Amplitude and Phase
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dft/fft.html">
   Fast Fourier Transform
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dft/convolution.html">
   Convolution and Filtering
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Jupyter Notebooks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/01_linear_systems.html">
   Linear Systems of Equations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/02_LU_decomposition.html">
   LU Decomposition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/03_polynomial_interpolation.html">
   Polynomial Interpolation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/04_spline_interpolation.html">
   Natural Cubic Spline Interpolation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/05_finite_difference_method.html">
   Finite Difference Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/06_least_squares_regression.html">
   Fitting Models to Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/07_pca.html">
   Principal Component Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/08_deblurring_images.html">
   Deblurring Images
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/09_computed_tomography.html">
   Computed Tomography
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/10_computing_eigenvalues.html">
   Computing Eigenvalues
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/11_pagerank.html">
   PageRank
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/12_fft.html">
   Discrete Fourier Transform
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix/complex_numbers.html">
   Complex Numbers, Vectors and Matrices
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix/matrix_multiplication.html">
   Matrix Multiplication
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix/norms.html">
   Vector and Matrix Norms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../appendix/inner_product.html">
   Inner Product
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/eigenvalues/computing_eigenvalues.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/UBCMath/math307"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#power-method">
   Power Method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rayleigh-quotient">
   Rayleigh Quotient
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inverse-iteration">
   Inverse Iteration
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pagerank">
   PageRank
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="computing-eigenvalues">
<h1>Computing Eigenvalues<a class="headerlink" href="#computing-eigenvalues" title="Permalink to this headline">¶</a></h1>
<div class="bigidea docutils">
<p>It is not practical to compute eigenvalues of a matrix <span class="math notranslate nohighlight">\(A\)</span> by finding roots of the characteristic polynomial <span class="math notranslate nohighlight">\(c_A(x)\)</span>. Instead, there are several efficient algorithms for numerically approximating eigenvalues without using <span class="math notranslate nohighlight">\(c_A(x)\)</span> such as the power method.</p>
</div>
<div class="section" id="power-method">
<h2>Power Method<a class="headerlink" href="#power-method" title="Permalink to this headline">¶</a></h2>
<div class="definition docutils">
<p>Let <span class="math notranslate nohighlight">\(A\)</span> be a square matrix. An eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span> of <span class="math notranslate nohighlight">\(A\)</span> is called a <strong>dominant eigenvalue</strong> if <span class="math notranslate nohighlight">\(\lambda\)</span> has (algebraic) multiplicity 1 and <span class="math notranslate nohighlight">\(| \lambda | &gt; | \mu |\)</span> for all other eigenvalues <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
</div>
<div class="definition docutils">
<p>Let <span class="math notranslate nohighlight">\(A\)</span> be an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix with eigenvalues <span class="math notranslate nohighlight">\(\lambda_1,\dots,\lambda_n\)</span> and corresponding eigenvectors <span class="math notranslate nohighlight">\(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n\)</span> with dominant eigenvalue <span class="math notranslate nohighlight">\(\lambda_1\)</span>. Let <span class="math notranslate nohighlight">\(\boldsymbol{x}_0\)</span> be any vector which is a linear combination of the eigenvectors of <span class="math notranslate nohighlight">\(A\)</span></p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{x}_0 = c_1 \boldsymbol{v}_1 + \cdots + c_n \boldsymbol{v}_n
\]</div>
<p>such that <span class="math notranslate nohighlight">\(c_1 \not= 0\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
A^k \boldsymbol{x}_0 = c_1 \lambda_1^k \boldsymbol{v}_1 + \cdots + c_n \lambda_n^k \boldsymbol{v}_n
\]</div>
<p>and therefore</p>
<div class="math notranslate nohighlight">
\[
(1/\lambda_1^k)A^k \boldsymbol{x}_0 = c_1 \boldsymbol{v}_1 + c_2 (\lambda_2/\lambda_1)^k \boldsymbol{v}_2 + \cdots + c_n (\lambda_n/\lambda_1)^k \boldsymbol{v}_n
\ \ \rightarrow \ \ c_1 \boldsymbol{v}_1 \ \ \text{as } k \to \infty
\]</div>
<p>because each term <span class="math notranslate nohighlight">\(| \lambda_i/\lambda_1 | &lt; 1\)</span> and so <span class="math notranslate nohighlight">\(\lambda_i/\lambda_1 \to 0\)</span> as <span class="math notranslate nohighlight">\(k \to \infty\)</span>. This method of approximating <span class="math notranslate nohighlight">\(\boldsymbol{v}_1\)</span> is called <strong>power iteration</strong> (or the <strong>power method</strong>).</p>
</div>
<div class="note docutils">
<p>The entries in the vector <span class="math notranslate nohighlight">\(A^k \boldsymbol{x}_0\)</span> may get very large as <span class="math notranslate nohighlight">\(k\)</span> increases therefore it is helpful to normalize at each step. The simplest way is to divide by <span class="math notranslate nohighlight">\(\| \boldsymbol{x}_k \|_{\infty} = \max \{\boldsymbol{x}_k \}\)</span></p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{x}_{k+1} = \frac{A \boldsymbol{x}_k}{\| A \boldsymbol{x}_k \|_{\infty}}
\]</div>
<p>This is called <strong>normalized power iteration</strong>. Note that <span class="math notranslate nohighlight">\(\| A \boldsymbol{x}_k \|_{\infty}\)</span> gives an approximation of <span class="math notranslate nohighlight">\(\lambda_1\)</span> at each step.</p>
</div>
<div class="example docutils">
<p>Approximate the dominant eigenvalue and eigenvector of the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \end{bmatrix}
\end{split}\]</div>
<p>by 4 iterations of the normalized power method. Choose a random starting vector</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{x}_0 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}
\end{split}\]</div>
<p>and compute</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
A \boldsymbol{x}_0 &amp;=
\begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \end{bmatrix}
\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}
=
\begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}
&amp;
\boldsymbol{x}_1 = \frac{A \boldsymbol{x}_0}{\| A \boldsymbol{x}_0 \|_{\infty}}
&amp;=
\begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}
\\
A \boldsymbol{x}_1 &amp;=
\begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \end{bmatrix}
\begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}
=
\begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix}
&amp;
\boldsymbol{x}_2 = \frac{A \boldsymbol{x}_1}{\| A \boldsymbol{x}_1 \|_{\infty}}
&amp;=
\begin{bmatrix} 1 \\ 1 \\ 0.5 \end{bmatrix}
\\
A \boldsymbol{x}_2 &amp;=
\begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \end{bmatrix}
\begin{bmatrix} 1 \\ 1 \\ 0.5 \end{bmatrix}
=
\begin{bmatrix} 2 \\ 2.5 \\ 1.5 \end{bmatrix}
&amp;
\boldsymbol{x}_3 = \frac{A \boldsymbol{x}_2}{\| A \boldsymbol{x}_2 \|_{\infty}}
&amp;=
\begin{bmatrix} 0.8 \\ 1 \\ 0.6 \end{bmatrix}
\\
A \boldsymbol{x}_3 &amp;=
\begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \end{bmatrix}
\begin{bmatrix} 0.8 \\ 1 \\ 0.6 \end{bmatrix}
=
\begin{bmatrix} 1.8 \\ 2.4 \\ 1.6 \end{bmatrix}
&amp;
\boldsymbol{x}_4 = \frac{A \boldsymbol{x}_3}{\| A \boldsymbol{x}_3 \|_{\infty}}
&amp;=
\begin{bmatrix} 0.75 \\ 1 \\ 0.67 \end{bmatrix}
\end{align*}
\end{split}\]</div>
<p>Therefore we get approximations</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\lambda_1 \approx 2.4
\hspace{10mm}
\boldsymbol{v}_1 \approx \begin{bmatrix} 0.75 \\ 1 \\ 0.67 \end{bmatrix}
\end{split}\]</div>
<p>The actual dominant eigenvector is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{v}_1 = \begin{bmatrix} 1/\sqrt{2} \\ 1 \\ 1/\sqrt{2} \end{bmatrix}
\end{split}\]</div>
<p>and we verify</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \end{bmatrix}
\begin{bmatrix} 1/\sqrt{2} \\ 1 \\ 1/\sqrt{2} \end{bmatrix}
=
\begin{bmatrix} \frac{1 + \sqrt{2}}{\sqrt{2}} \\ 1 + \sqrt{2} \\ \frac{1 + \sqrt{2}}{\sqrt{2}} \end{bmatrix}
=
(1 + \sqrt{2}) \begin{bmatrix} \frac{1}{\sqrt{2}} \\ 1 \\ \frac{1}{\sqrt{2}} \end{bmatrix}
\end{split}\]</div>
<p>therefore <span class="math notranslate nohighlight">\(\lambda_1 \approx 2.4142\)</span>.</p>
</div>
</div>
<div class="section" id="rayleigh-quotient">
<h2>Rayleigh Quotient<a class="headerlink" href="#rayleigh-quotient" title="Permalink to this headline">¶</a></h2>
<div class="definition docutils">
<p>Note that if <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is an eigenvector of a matrix <span class="math notranslate nohighlight">\(A\)</span> with eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span> then</p>
<div class="math notranslate nohighlight">
\[
A \boldsymbol{x} = \lambda \boldsymbol{x}
\hspace{5mm}
\Rightarrow
\hspace{5mm}
\boldsymbol{x}^T A \boldsymbol{x} = \boldsymbol{x}^T (\lambda \boldsymbol{x})
\hspace{5mm}
\Rightarrow
\hspace{5mm}
\lambda = \frac{\boldsymbol{x}^T A \boldsymbol{x}}{ \boldsymbol{x}^T \boldsymbol{x} }
\]</div>
<p>Therefore if <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is an approximate eigenvector of <span class="math notranslate nohighlight">\(A\)</span> then an approximation of the corresponding eigenvalue is given by the <strong>Rayleigh quotient</strong></p>
<div class="math notranslate nohighlight">
\[
\frac{\boldsymbol{x}^T A \boldsymbol{x}}{ \boldsymbol{x}^T \boldsymbol{x} }
\]</div>
<p>In particular, in the power method, the sequence of Rayleigh quotients</p>
<div class="math notranslate nohighlight">
\[
\lambda_k = \frac{\boldsymbol{x}_k^T A \boldsymbol{x}_k}{ \boldsymbol{x}_k^T \boldsymbol{x}_k }
\]</div>
<p>converges to the dominant eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
</div>
</div>
<div class="section" id="inverse-iteration">
<h2>Inverse Iteration<a class="headerlink" href="#inverse-iteration" title="Permalink to this headline">¶</a></h2>
<div class="definition docutils">
<p>Let <span class="math notranslate nohighlight">\(A\)</span> be an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix with eigenvalues <span class="math notranslate nohighlight">\(\lambda_1,\dots,\lambda_n\)</span> (in increasing order <span class="math notranslate nohighlight">\(\lambda_1 &lt; \lambda_2 &lt; \cdots &lt; \lambda_n\)</span>) with corresponding eigenvectors <span class="math notranslate nohighlight">\(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n\)</span>. Then <span class="math notranslate nohighlight">\(1/\lambda_1,\dots,1/\lambda_n\)</span> are the eigenvalues of <span class="math notranslate nohighlight">\(A^{-1}\)</span> (in decreasing order) with corresponding eigenvectors <span class="math notranslate nohighlight">\(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n\)</span>. <strong>Inverse iteration</strong> is power iteration applied to <span class="math notranslate nohighlight">\(A^{-1}\)</span> to find the dominant eigenvalue <span class="math notranslate nohighlight">\(1/\lambda_n\)</span> of <span class="math notranslate nohighlight">\(A^{-1}\)</span> (equivalently, the smallest eigenvalue <span class="math notranslate nohighlight">\(\lambda_n\)</span> of <span class="math notranslate nohighlight">\(A\)</span>) with eigenvector <span class="math notranslate nohighlight">\(\boldsymbol{v}_n\)</span>. At each step, solve the system and normalize</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y}_{k+1} = A^{-1} \boldsymbol{x}_k \ \ \Rightarrow \ \ A \boldsymbol{y}_{k+1} = \boldsymbol{x}_k \ \ \Rightarrow \ \ \boldsymbol{x}_{k+1} = \frac{\boldsymbol{y}_{k+1}}{\| \boldsymbol{y}_{k+1} \|_{\infty}}
\]</div>
</div>
<div class="example docutils">
<p>Compute 2 steps of inverse iterations for the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 1 &amp; 2 &amp; 0 \\ 1 &amp; 0 &amp; 3 \end{bmatrix}
\end{split}\]</div>
<p>Since we are going to repeatedly solve systems <span class="math notranslate nohighlight">\(A \boldsymbol{x} = \boldsymbol{b}\)</span>, we should find the LU decomposition and use forward and backward substitution</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\left[ \begin{array}{rrr} 1 &amp; 0 &amp; 0 \\ -1 &amp; 1 &amp; 0 \\ -1 &amp; 0 &amp; 1 \end{array} \right]
\begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 1 &amp; 2 &amp; 0 \\ 1 &amp; 0 &amp; 3 \end{bmatrix}
&amp;=
\left[ \begin{array}{rrr} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; -1 \\ 0 &amp; -1 &amp; 2 \end{array} \right] \\
\left[ \begin{array}{rrr} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \end{array} \right]
\left[ \begin{array}{rrr} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; -1 \\ 0 &amp; -1 &amp; 2 \end{array} \right]
&amp;=
\left[ \begin{array}{rrr} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; -1 \\ 0 &amp; \phantom{+}0 &amp; 1 \end{array} \right] \\
\end{align*}
\end{split}\]</div>
<p>Therefore</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = LU =
\left[ \begin{array}{rrr} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; -1 &amp; \phantom{+}1 \end{array} \right]
\left[ \begin{array}{rrr} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; -1 \\ 0 &amp; \phantom{+}0 &amp; 1 \end{array} \right]
\end{split}\]</div>
<p>and in fact we see that <span class="math notranslate nohighlight">\(A\)</span> is positive definite and this is the Cholesky decomposition. Choose a random starting vector</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{x}_0 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}
\end{split}\]</div>
<p>and compute</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\left[ \begin{array}{rrr} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; -1 &amp; \phantom{+}1 \end{array} \right] \boldsymbol{z}_1 &amp;=  \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}
&amp; \boldsymbol{z}_1 &amp;= \left[ \begin{array}{r} 1 \\ -1 \\ -2 \end{array} \right] \\
\left[ \begin{array}{rrr} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; -1 \\ 0 &amp; \phantom{+}0 &amp; 1 \end{array} \right] \boldsymbol{y}_1 &amp;= \left[ \begin{array}{r} 1 \\ -1 \\ -2 \end{array} \right]
&amp; \boldsymbol{y}_1 &amp;= \left[ \begin{array}{r} 6 \\ -3 \\ -2 \end{array} \right]
&amp; \boldsymbol{x}_1 &amp;= \left[ \begin{array}{c} 1 \\ -1/2 \\ -1/3 \end{array} \right] \\
\left[ \begin{array}{rrr} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; -1 &amp; \phantom{+}1 \end{array} \right] \boldsymbol{z}_2 &amp;= \left[ \begin{array}{c} 1 \\ -1/2 \\ -1/3 \end{array} \right]
&amp; \boldsymbol{z}_2 &amp;= \left[ \begin{array}{c} 1 \\ -3/2 \\ -17/6 \end{array} \right] \\
\left[ \begin{array}{rrr} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; -1 \\ 0 &amp; \phantom{+}0 &amp; 1 \end{array} \right] \boldsymbol{y}_2 &amp;= \left[ \begin{array}{c} 1 \\ -3/2 \\ -17/6 \end{array} \right]
&amp; \boldsymbol{y}_2 &amp;= \left[ \begin{array}{c} 49/6 \\ -13/3 \\ -17/6 \end{array} \right]
&amp; \boldsymbol{x}_2 &amp;= \left[ \begin{array}{c} 1 \\ -26/49 \\ -17/49 \end{array} \right]
\end{align*}
\end{split}\]</div>
<p>Our approximation of the eigenvector of <span class="math notranslate nohighlight">\(A\)</span> corresponding to the smallest eigenvalue is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{v} = \left[ \begin{array}{c} 1 \\ -26/49 \\ -17/49 \end{array} \right]
\approx
\left[ \begin{array}{r} 1.00 \\ -0.53 \\ -0.35 \end{array} \right]
\end{split}\]</div>
<p>with eigenvalue <span class="math notranslate nohighlight">\(\lambda \approx 0.12\)</span> given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 1 &amp; 2 &amp; 0 \\ 1 &amp; 0 &amp; 3 \end{bmatrix}
\left[ \begin{array}{r} 1.00 \\ -0.53 \\ -0.35 \end{array} \right]
=
\left[ \begin{array}{r} 0.12 \\ -0.06 \\ -0.05 \end{array} \right]
=
0.12 \left[ \begin{array}{r} 1.00 \\ -0.50 \\ -0.42 \end{array} \right]
\end{split}\]</div>
<p>The actual eigenvector is approximately</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{v} \approx \left[ \begin{array}{r} 1.000 \\ -0.532 \\ -0.347 \end{array} \right]
\end{split}\]</div>
<p>with eigenvalue</p>
<div class="math notranslate nohighlight">
\[
\lambda \approx 0.12061476
\]</div>
</div>
</div>
<div class="section" id="pagerank">
<h2>PageRank<a class="headerlink" href="#pagerank" title="Permalink to this headline">¶</a></h2>
<div class="bigidea docutils">
<p>The PageRank vector is the dominant eigenvector of the adjacency matrix of a directed graph and it ranks the importance of each vertex in the graph.</p>
</div>
<div class="definition docutils">
<p>Consider a directed graph <span class="math notranslate nohighlight">\(G\)</span> with <span class="math notranslate nohighlight">\(N\)</span> vertices (see <a class="reference external" href="https://en.wikipedia.org/wiki/Directed_graph">Wikipedia:Directed graph</a>). The <strong>adjacency matrix</strong> is the <span class="math notranslate nohighlight">\(N \times N\)</span> matrix <span class="math notranslate nohighlight">\(A = [a_{i,j}]\)</span> where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
a_{i,j} = \left\{ \begin{array}{cc} 1 &amp; \text{ if there is an edge to vertex $i$ from vertex $j$} \\ 0 &amp; \text{ if not } \end{array} \right.
\end{split}\]</div>
<p>Suppose the vertices of <span class="math notranslate nohighlight">\(G\)</span> represent a collection webpages and the edges represent links from one webpage to another. (We only count one link maximum from one webpage to another and no links from a webpage to itself.) The <strong>stochastic matrix</strong> of <span class="math notranslate nohighlight">\(G\)</span> represents the process of clicking a random link on a webpage and is given by <span class="math notranslate nohighlight">\(P = [p_{i,j}]\)</span> where</p>
<div class="math notranslate nohighlight">
\[
p_{i,j} = \frac{a_{i,j}}{\text{total $\#$ of links from webpage $j$}}
\]</div>
<p>The entry <span class="math notranslate nohighlight">\(p_{i,j}\)</span> is the probability of clicking to webpage <span class="math notranslate nohighlight">\(i\)</span> from webpage <span class="math notranslate nohighlight">\(j\)</span>.</p>
</div>
<div class="example docutils">
<p>Consider the directed graph</p>
<a class="reference internal image-reference" href="../_images/graph01.png"><img alt="../_images/graph01.png" class="align-center" src="../_images/graph01.png" style="width: 300px;" /></a>
<p>Construct the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> and the stochastic matrix <span class="math notranslate nohighlight">\(P\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 1 &amp; 0 \end{bmatrix}
\hspace{10mm}
P = \begin{bmatrix} 0 &amp; 1/2 &amp; 1/3 &amp; 0 \\ 1 &amp; 0 &amp; 1/3 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1/2 &amp; 1/3 &amp; 0 \end{bmatrix}
\end{split}\]</div>
</div>
<div class="definition docutils">
<p>The <strong>Google matrix</strong> of a directed graph <span class="math notranslate nohighlight">\(G\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\alpha P + (1 - \alpha) \boldsymbol{v} \boldsymbol{e}^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(P\)</span> is the stochastic matrix of <span class="math notranslate nohighlight">\(G\)</span>, <span class="math notranslate nohighlight">\(0 &lt; \alpha &lt; 1\)</span> is the <strong>teleportation parameter</strong>, <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span> is the <strong>teleportation distribution vector</strong> and <span class="math notranslate nohighlight">\(\boldsymbol{e}^T = \begin{bmatrix} 1 &amp; \cdots &amp; 1 \end{bmatrix}\)</span> is a vector of 1s. Note <span class="math notranslate nohighlight">\(\boldsymbol{v} \boldsymbol{e}^T\)</span> is the matrix with vector <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span> in every column. See <a class="reference external" href="https://www.cs.purdue.edu/homes/dgleich/publications.html">PageRank Beyond the Web</a> and  <a class="reference external" href="https://en.wikipedia.org/wiki/PageRank">Wikipedia:PageRank</a></p>
</div>
<div class="note docutils">
<p>The teleportation vector <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span> has entries between 0 and 1 and the entries sum to 1. In other words, it is a stochastic vector. The vector <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span> is usually chosen to be <span class="math notranslate nohighlight">\(\boldsymbol{v} = (1/N)\boldsymbol{e}\)</span> where <span class="math notranslate nohighlight">\(N\)</span> is the number of vertices in the graph. The stochastic matrix <span class="math notranslate nohighlight">\(\boldsymbol{v} \boldsymbol{e}^T\)</span> then represents the process of transitioning to a random webpage with uniform probability. The Google matrix is a stochastic matrix which represents the process: at each step, do either:</p>
<ul class="simple">
<li><p>probability <span class="math notranslate nohighlight">\(\alpha\)</span>: click a random link on the webpage to visit another webapge</p></li>
<li><p>probability <span class="math notranslate nohighlight">\(1 - \alpha\)</span>: teleport to any webpage according to the distribution <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span></p></li>
</ul>
<p>The teleportation parameter <span class="math notranslate nohighlight">\(\alpha\)</span> is usually chosen to be <span class="math notranslate nohighlight">\(\alpha = 0.85\)</span>.</p>
</div>
<div class="theorem docutils">
<p>Let <span class="math notranslate nohighlight">\(G\)</span> be a directed graph and let <span class="math notranslate nohighlight">\(P\)</span> be the stochastic matrix for <span class="math notranslate nohighlight">\(G\)</span>. Choose parameters <span class="math notranslate nohighlight">\(0 &lt; \alpha &lt; 0\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span>. There exists a unique steady state vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> (with entries between 0 and 1 and the entries sum to 1) such that</p>
<div class="math notranslate nohighlight">
\[
\left( \alpha P + (1 - \alpha) \boldsymbol{v} \boldsymbol{e}^T \right) \boldsymbol{x} = \boldsymbol{x}
\]</div>
<p>The vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is called the <strong>PageRank vector</strong> and the entry <span class="math notranslate nohighlight">\(x_i\)</span> is the PageRank of the webpage at vertex <span class="math notranslate nohighlight">\(i\)</span>. The Google search result lists the webpages in order of their PageRank.</p>
</div>
<div class="note docutils">
<p>A directed graph <span class="math notranslate nohighlight">\(G\)</span> represents a collection of webpages that contain the words in a Google search. The PageRank vector ranks the importance of the webpages for the search. There are usually hundreds of millions webpages in the graph therefore the Google matrix is HUGE! But the founders of Google showed that the power iteration algorithm converges well enough after about 50 iterations to find the webpages with the top PageRank.</p>
</div>
<div class="example docutils">
<p>Find the Google matrix for the directed graph in the example above for <span class="math notranslate nohighlight">\(\alpha = 0.85\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{v} = (1/N) \boldsymbol{e}\)</span>. Compute</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\alpha P + (1 - \alpha) \boldsymbol{v} \boldsymbol{e}^T &amp;=
0.85 \begin{bmatrix} 0 &amp; 1/2 &amp; 1/3 &amp; 0 \\ 1 &amp; 0 &amp; 1/3 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1/2 &amp; 1/3 &amp; 0 \end{bmatrix}
+
\frac{0.15}{4}
\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 \end{bmatrix} \\
&amp;=
\begin{bmatrix}
0.0375 &amp; 0.4625 &amp; 0.3208 &amp; 0.0375 \\
0.8875 &amp; 0.0375 &amp; 0.3208 &amp; 0.8875 \\
0.0375 &amp; 0.0375 &amp; 0.0375 &amp; 0.0375 \\
0.0375 &amp; 0.4625 &amp; 0.3208 &amp; 0.0375
\end{bmatrix}
\end{align*}
\end{split}\]</div>
<p>Compute 50 iterations of the power method to approximate the PageRank vector</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{x} \approx \begin{bmatrix} 0.2472 \\ 0.4681 \\ 0.0375 \\ 0.2472 \end{bmatrix}
\end{split}\]</div>
<p>Clearly, vertex 2 is the most important in the graph.</p>
</div>
</div>
<div class="section" id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<ol>
<li><p>Determine whether the statement is <strong>True</strong> or <strong>False</strong>.</p>
<ul class="simple">
<li><p>The inverse iteration algorithm (without normalization) computes a recursive sequence <span class="math notranslate nohighlight">\( A \boldsymbol{x}_{k+1} = \boldsymbol{x}_k \)</span> where <span class="math notranslate nohighlight">\( \boldsymbol{x}_k \)</span> converges to:</p>
<ul>
<li><p>the largest (in absolute value <span class="math notranslate nohighlight">\( | \lambda | \)</span>) eigenvalue of <span class="math notranslate nohighlight">\( A \)</span></p></li>
<li><p>an eigenvector corresponding to the largest (in absolute value <span class="math notranslate nohighlight">\( | \lambda | \)</span>) eigenvalue of <span class="math notranslate nohighlight">\( A \)</span></p></li>
<li><p>the smallest (in absolute value <span class="math notranslate nohighlight">\( | \lambda | \)</span>) eigenvalue of <span class="math notranslate nohighlight">\( A \)</span></p></li>
<li><p>an eigenvector corresponding to the smallest (in absolute value <span class="math notranslate nohighlight">\( | \lambda | \)</span>) eigenvalue of <span class="math notranslate nohighlight">\( A \)</span></p></li>
</ul>
</li>
<li><p>In the power iteration algorithm, we divide by <span class="math notranslate nohighlight">\( \| A \boldsymbol{x}_k \|_{\infty} \)</span> in each step to:</p>
<ul>
<li><p>make the algorithm run faster</p></li>
<li><p>prevent the entries of the vectors <span class="math notranslate nohighlight">\( \boldsymbol{x}_k \)</span> from becoming too large/small</p></li>
<li><p>produce a more accurate result</p></li>
</ul>
</li>
<li><p>It is necessary to compute all the eigenvectors of the Google matrix to find the PageRank vector of a directed graph.</p></li>
</ul>
</li>
<li><p>Let <span class="math notranslate nohighlight">\( A \)</span> be a <span class="math notranslate nohighlight">\( 2 \times 2 \)</span> matrix with eigenvalues <span class="math notranslate nohighlight">\( \lambda_1 = 1 \)</span> and <span class="math notranslate nohighlight">\( \lambda_2 = 1/2 \)</span> and corresponding eigenvectors</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \boldsymbol{v}_1 = \begin{bmatrix} 1 \\ 2 \end{bmatrix} \hspace{10mm} \boldsymbol{v}_2 = \begin{bmatrix} -1 \\ 1 \end{bmatrix}
    \end{split}\]</div>
<p>If we choose <span class="math notranslate nohighlight">\( \boldsymbol{x}_0 = \begin{bmatrix} 1 \\ 5 \end{bmatrix} \)</span> then the sequence <span class="math notranslate nohighlight">\( \boldsymbol{x}_{k+1} = A \boldsymbol{x}_k \)</span> converges to what?</p>
</li>
<li><p>Consider the same directed graph as in the example in the section on PageRank:</p>
<a class="reference internal image-reference" href="../_images/graph01.png"><img alt="../_images/graph01.png" class="align-center" src="../_images/graph01.png" style="width: 300px;" /></a>
<p>As <span class="math notranslate nohighlight">\(\alpha \to 1\)</span>, describe what happens to the PageRank <span class="math notranslate nohighlight">\(x_3\)</span> of vertex 3.</p>
</li>
<li><p>Let <span class="math notranslate nohighlight">\(G\)</span> be the complete directed graph with <span class="math notranslate nohighlight">\(N\)</span> vertices. In other words, there is an edge from each vertex to every other vertex in <span class="math notranslate nohighlight">\(G\)</span> (excluding edges from a vertex to itself). Describe the Google matrix and the PageRank vector for the complete directed graph.</p></li>
<li><p>Find the Google matrix <span class="math notranslate nohighlight">\(\alpha P + (1 - \alpha) \boldsymbol{v} \boldsymbol{e}^T\)</span> for the directed graph</p>
<a class="reference internal image-reference" href="../_images/graph02.png"><img alt="../_images/graph02.png" class="align-center" src="../_images/graph02.png" style="width: 300px;" /></a>
<p>using teleportation parameter <span class="math notranslate nohighlight">\(\alpha=0.5\)</span> and uniform distribution vector <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span>. Let <span class="math notranslate nohighlight">\(\boldsymbol{x}_0 = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}^T\)</span> and use Python to compute 50 iterations of the power method to approximate the PageRank vector.</p>
</li>
<li><p>Find the Google matrix <span class="math notranslate nohighlight">\(\alpha P + (1 - \alpha) \boldsymbol{v} \boldsymbol{e}^T\)</span> for the directed graph in the previous exercise using teleportation parameter <span class="math notranslate nohighlight">\(\alpha=0.8\)</span> and distribution vector <span class="math notranslate nohighlight">\(\boldsymbol{v} =  \begin{bmatrix} 0 &amp; 1/2 &amp; 1/2 &amp; 0 \end{bmatrix}^T\)</span>. Let <span class="math notranslate nohighlight">\(\boldsymbol{x}_0 = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}^T\)</span> and use Python to compute 50 iterations of the power method to approximate the PageRank vector.</p></li>
</ol>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./eigenvalues"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="svd.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Singular Value Decomposition</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="../dft/dft.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Discrete Fourier Transform</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By UBC Math<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>